import os
import requests
import json
import urllib.parse
import random
from tavily import TavilyClient
from groq import Groq

# å¾ Secrets è®€å– API Key
TAVILY_API_KEY = os.environ.get("TAVILY_API_KEY")
GROQ_API_KEY = os.environ.get("GROQ_API_KEY")

def get_gmap_link(location_name):
    """ç”¢ç”Ÿ Google Maps é€£çµ"""
    query = urllib.parse.quote(location_name)
    return f"https://www.google.com/maps/search/?api=1&query={query}"

def analyze_with_ai(text_content, source):
    """
    å‘¼å« Groq AI é€²è¡Œé–±è®€èˆ‡èƒå–
    """
    if not GROQ_API_KEY: return []
    
    client = Groq(api_key=GROQ_API_KEY)
    print(f"ğŸ§  [Groq AI] æ­£åœ¨åˆ†æ ({source})...")
    
    prompt = f"""
    ä½ æ˜¯ä¸€å€‹è³‡æ–™èƒå–æ©Ÿå™¨äººã€‚è«‹é–±è®€ä»¥ä¸‹è³‡æ–™ï¼Œæ‰¾å‡ºæ¨è–¦çš„ã€Œåº—å®¶åç¨±ã€ã€‚
    
    è³‡æ–™ä¾†æº ({source})ï¼š
    {text_content}
    
    è«‹åš´æ ¼éµå®ˆä»¥ä¸‹è¦å‰‡ï¼š
    1. å›å‚³ JSON é™£åˆ—ï¼Œæ ¼å¼ç‚ºï¼š
       [{{
           "name": "åº—å", 
           "address": "åº—å®¶åœ°å€(å¦‚æœæ–‡ç« æœ‰å¯«ï¼Œæ²’å¯«è«‹å›å‚³ç©ºå­—ä¸²)", 
           "summary": "15å­—ä»¥å…§çš„ç‰¹è‰²çŸ­è©•"
       }}]
    2. è‡³å°‘æŠ“ 5 é–“ã€‚
    3. åªè¦ JSONï¼Œä¸è¦å»¢è©±ã€‚
    4. å¦‚æœæ‰¾ä¸åˆ°ï¼Œå›å‚³ []ã€‚
    """

    try:
        chat_completion = client.chat.completions.create(
            messages=[{"role": "user", "content": prompt}],
            model="llama-3.3-70b-versatile",
        )
        ai_response = chat_completion.choices[0].message.content
        
        # è§£æ JSON
        start_idx = ai_response.find('[')
        end_idx = ai_response.rfind(']') + 1
        
        if start_idx != -1 and end_idx != -1:
            return json.loads(ai_response[start_idx:end_idx])
        return []
    except Exception as e:
        print(f"âš ï¸ AI åˆ†æéŒ¯èª¤: {e}")
        return []

def scrape_web(keyword):
    print(f"\nğŸš€ [ç³»çµ±] æ”¶åˆ° LINE è«‹æ±‚ï¼Œç›®æ¨™ï¼š{keyword}")
    
    if not TAVILY_API_KEY or not GROQ_API_KEY:
        print("âŒ éŒ¯èª¤ï¼šè«‹ç¢ºèª Secrets è£¡æœ‰ TAVILY_API_KEY å’Œ GROQ_API_KEY")
        return []

    tavily = TavilyClient(api_key=TAVILY_API_KEY)
    
    # 1. é»‘åå–® (é¿é–‹ç„¡æ³•è®€å–çš„ç¶²ç«™)
    blacklist_domains = [
        "instagram.com", "facebook.com", "youtube.com", "tiktok.com", 
        "twitter.com", "threads.net", "dcard.tw",
        "trip.com", "klook.com", "kkday.com", "agoda.com", "booking.com"
    ]

    # æœå°‹ç­–ç•¥
    random_terms = ["æ¨è–¦", "å¿…åƒ", "æ‡¶äººåŒ…", "é£Ÿè¨˜", "è©•åƒ¹", "æ’è¡Œæ¦œ"]
    search_term = f"{keyword} {random.choice(random_terms)}"
    print(f"ğŸ” [Tavily] æ­£åœ¨æœå°‹ï¼š{search_term} ...")

    try:
        # ä¸€æ¬¡æŠ“ 10 ç¯‡å›ä¾†ç•¶å€™è£œ
        search_result = tavily.search(
            query=search_term, 
            search_depth="basic", 
            max_results=10, 
            exclude_domains=blacklist_domains
        )
    except Exception as e:
        print(f"âŒ æœå°‹å¤±æ•—: {e}")
        return []

    if not search_result['results']:
        return [] # çœŸçš„æ²’æ±è¥¿å°±å›å‚³ç©º

    # æº–å‚™æ–‡ç« æ± 
    articles_pool = search_result['results']
    random.shuffle(articles_pool)
    
    # ==========================================
    # ğŸ”„ è¿´åœˆé‡è©¦æ©Ÿåˆ¶ (æœ€å¤š 3 æ¬¡)
    # ==========================================
    max_retries = 3
    final_shops = []

    for attempt in range(1, max_retries + 1):
        if not articles_pool:
            break
            
        current_article = articles_pool.pop()
        print(f"ğŸ¬ [ç¬¬ {attempt} æ¬¡å˜—è©¦] è®€å–ï¼š{current_article['title']}")
        
        jina_url = f"https://r.jina.ai/{current_article['url']}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        
        try:
            page = requests.get(jina_url, headers=headers, timeout=8)
            
            if page.status_code == 200 and len(page.text) > 500:
                found_shops = analyze_with_ai(page.text[:8000], source="å…¨æ–‡é–±è®€")
                
                if found_shops:
                    print(f"ğŸ‰ æˆåŠŸæŠ“åˆ° {len(found_shops)} é–“åº—ï¼")
                    final_shops = found_shops
                    break
            else:
                print(f"âš ï¸ è®€å–å¤±æ•— Code: {page.status_code}")
        except Exception as e:
            print(f"âš ï¸ é€£ç·šéŒ¯èª¤: {e}")

    # ==========================================
    # ğŸ›¡ï¸ Bè¨ˆç•«ï¼šæœå°‹æ‘˜è¦æ•‘å ´
    # ==========================================
    if not final_shops:
        print("ğŸ›¡ï¸ [Bè¨ˆç•«] å•Ÿå‹•ï¼æ”¹ç”¨ã€Œæœå°‹æ‘˜è¦ã€åˆ†æ...")
        snippets_text = ""
        for item in search_result['results']:
            snippets_text += f"æ¨™é¡Œï¼š{item['title']}\næ‘˜è¦ï¼š{item['content']}\n\n"
            
        final_shops = analyze_with_ai(snippets_text, source="æœå°‹æ‘˜è¦")

    # ==========================================
    # ğŸ“Š æ•´ç†æœ€çµ‚çµæœ
    # ==========================================
    if final_shops:
        # éš¨æ©Ÿé¸ 5 é–“
        selected = random.sample(final_shops, min(5, len(final_shops)))
        
        results = []
        for shop in selected:
            # è™•ç†é¡¯ç¤ºæ–‡å­—
            raw_address = shop.get('address', '').strip()
            summary = shop.get('summary', 'ç¶²å‹æ¨è–¦ç¾é£Ÿ')
            
            if len(raw_address) > 2:
                display_text = f"ğŸ“ {raw_address} | ğŸ“ {summary}"
            else:
                display_text = f"ğŸ“ {summary}"

            # æˆªæ–·éé•·çš„æ–‡å­—
            if len(display_text) > 60:
                display_text = display_text[:57] + "..."

            results.append({
                "name": shop['name'],
                "address": display_text,     
                "score": "ç²¾é¸",             
                "image": "https://images.unsplash.com/photo-1555939594-58d7cb561ad1", # çµ±ä¸€ç”¨é€™å¼µç¾é£Ÿåœ–
                "link": get_gmap_link(shop['name'])
            })
        return results
    else:
        return []
