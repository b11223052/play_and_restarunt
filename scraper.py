import os
import requests
import json
import urllib.parse
import random
import time
from tavily import TavilyClient
from groq import Groq

# è®€å– Keys
TAVILY_API_KEY = os.environ.get("TAVILY_API_KEY")
GROQ_API_KEY = os.environ.get("GROQ_API_KEY")

# å®šç¾©ä¸€å¼µé è¨­çš„å®‰å…¨åœ–ç‰‡ (Unsplash)
DEFAULT_IMAGE = "https://images.unsplash.com/photo-1555939594-58d7cb561ad1"

def get_gmap_link(location_name):
    query = urllib.parse.quote(location_name)
    return f"https://www.google.com/maps/search/?api=1&query={query}"

def analyze_with_ai(text_content, source):
    """
    å‘¼å« Groq AI é€²è¡Œé–±è®€èˆ‡èƒå–
    """
    if not GROQ_API_KEY: return []
    
    client = Groq(api_key=GROQ_API_KEY)
    print(f"ğŸ§  [Groq AI] æ­£åœ¨åˆ†æ ({source})...")
    
    # ğŸ”¥ ä¿®æ”¹é» 1: æ›´æ–° Promptï¼Œè«‹ AI é †ä¾¿æŠ“åœ–ç‰‡ç¶²å€
    prompt = f"""
    ä½ æ˜¯ä¸€å€‹è³‡æ–™èƒå–æ©Ÿå™¨äººã€‚è«‹é–±è®€ä»¥ä¸‹è³‡æ–™ï¼Œæ‰¾å‡ºæ¨è–¦çš„ã€Œåº—å®¶åç¨±ã€ã€‚
    
    è³‡æ–™ä¾†æº ({source})ï¼š
    {text_content}
    
    è«‹åš´æ ¼éµå®ˆä»¥ä¸‹è¦å‰‡ï¼š
    1. å›å‚³ JSON é™£åˆ—ï¼Œæ ¼å¼ç‚ºï¼š
       [{{
           "name": "åº—å", 
           "address": "åœ°å€(æ²’å¯«å›å‚³ç©ºå­—ä¸²)", 
           "summary": "15å­—ç‰¹è‰²çŸ­è©•",
           "image_url": "è«‹å¾æ–‡ç« ä¸­æ‰¾å‡ºä»£è¡¨è©²åº—é£Ÿç‰©çš„åœ–ç‰‡é€£çµ(ç¶²å€)ã€‚å¦‚æœæ‰¾ä¸åˆ°ã€æˆ–è€…æ˜¯Plan Bæ‘˜è¦æ¨¡å¼ï¼Œè«‹å›å‚³ null"
       }}]
    2. è‡³å°‘æŠ“ 5 é–“ã€‚
    3. åªè¦ JSONï¼Œä¸è¦å»¢è©±ã€‚
    4. å¦‚æœæ‰¾ä¸åˆ°ï¼Œå›å‚³ []ã€‚
    """

    try:
        chat_completion = client.chat.completions.create(
            messages=[{"role": "user", "content": prompt}],
            model="llama-3.3-70b-versatile",
        )
        ai_response = chat_completion.choices[0].message.content
        
        start_idx = ai_response.find('[')
        end_idx = ai_response.rfind(']') + 1
        
        if start_idx != -1 and end_idx != -1:
            return json.loads(ai_response[start_idx:end_idx])
        return []
    except Exception as e:
        print(f"âš ï¸ AI åˆ†æéŒ¯èª¤: {e}")
        return []

def scrape_web(keyword):
    print(f"\nğŸš€ [ç³»çµ±] æ”¶åˆ° LINE è«‹æ±‚ï¼Œç›®æ¨™ï¼š{keyword}")
    
    if not TAVILY_API_KEY or not GROQ_API_KEY:
        print("âŒ éŒ¯èª¤ï¼šè«‹ç¢ºèª Secrets è£¡æœ‰ Keys")
        return []

    tavily = TavilyClient(api_key=TAVILY_API_KEY)
    
    blacklist_domains = [
        "instagram.com", "facebook.com", "youtube.com", "tiktok.com", 
        "twitter.com", "threads.net", "dcard.tw",
        "trip.com", "klook.com", "kkday.com", "agoda.com", "booking.com"
    ]

    random_terms = ["æ¨è–¦", "å¿…åƒ", "æ‡¶äººåŒ…", "é£Ÿè¨˜", "è©•åƒ¹", "æ’è¡Œæ¦œ"]
    search_term = f"{keyword} {random.choice(random_terms)}"
    print(f"ğŸ” [Tavily] æ­£åœ¨æœå°‹ï¼š{search_term} ...")

    try:
        search_result = tavily.search(
            query=search_term, 
            search_depth="basic", 
            max_results=10, 
            exclude_domains=blacklist_domains
        )
    except Exception as e:
        print(f"âŒ æœå°‹å¤±æ•—: {e}")
        return []

    if not search_result['results']:
        return []

    articles_pool = search_result['results']
    random.shuffle(articles_pool)
    
    max_retries = 3
    final_shops = []

    # ==========================================
    # Plan A: å…¨æ–‡æ¨¡å¼ (æ¯”è¼ƒæœ‰æ©ŸæœƒæŠ“åˆ°åœ–)
    # ==========================================
    for attempt in range(1, max_retries + 1):
        if not articles_pool:
            break
            
        current_article = articles_pool.pop()
        print(f"ğŸ¬ [ç¬¬ {attempt} æ¬¡å˜—è©¦] è®€å–ï¼š{current_article['title']}")
        
        jina_url = f"https://r.jina.ai/{current_article['url']}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        
        try:
            page = requests.get(jina_url, headers=headers, timeout=8)
            
            if page.status_code == 200 and len(page.text) > 500:
                found_shops = analyze_with_ai(page.text[:8000], source="å…¨æ–‡é–±è®€")
                
                if found_shops:
                    print(f"ğŸ‰ æˆåŠŸæŠ“åˆ° {len(found_shops)} é–“åº—ï¼")
                    final_shops = found_shops
                    break
            else:
                print(f"âš ï¸ è®€å–å¤±æ•— Code: {page.status_code}")
        except Exception as e:
            print(f"âš ï¸ é€£ç·šéŒ¯èª¤: {e}")

    # ==========================================
    # Plan B: æ‘˜è¦æ¨¡å¼ (çµ•å°æ²’åœ–ï¼ŒAI æœƒå›å‚³ null)
    # ==========================================
    if not final_shops:
        print("ğŸ›¡ï¸ [Bè¨ˆç•«] å•Ÿå‹•ï¼æ”¹ç”¨ã€Œæœå°‹æ‘˜è¦ã€åˆ†æ...")
        snippets_text = ""
        for item in search_result['results']:
            snippets_text += f"æ¨™é¡Œï¼š{item['title']}\næ‘˜è¦ï¼š{item['content']}\n\n"
            
        final_shops = analyze_with_ai(snippets_text, source="æœå°‹æ‘˜è¦")

    # ==========================================
    # æ•´ç†çµæœ (æ±ºå®šè¦ç”¨å“ªå¼µåœ–)
    # ==========================================
    if final_shops:
        selected = random.sample(final_shops, min(5, len(final_shops)))
        
        results = []
        for shop in selected:
            raw_address = shop.get('address', '').strip()
            summary = shop.get('summary', 'ç¶²å‹æ¨è–¦ç¾é£Ÿ')
            
            # ğŸ”¥ ä¿®æ”¹é» 2: åœ–ç‰‡åˆ¤æ–·é‚è¼¯
            # 1. å–å¾— AI æŠ“åˆ°çš„åœ–
            ai_image = shop.get('image_url')
            
            # 2. æª¢æŸ¥åœ–ç‰‡æ˜¯å¦æœ‰æ•ˆ (ä¸æ˜¯ Noneï¼Œä¸”æ˜¯ http é–‹é ­)
            if ai_image and ai_image.startswith("http"):
                # é€™è£¡å¯ä»¥å†åŠ ä¸€å€‹å°åˆ¤æ–·ï¼Œå¦‚æœæ˜¯ .svg çµå°¾çš„é€šå¸¸æ˜¯ iconï¼Œä¸è¦ç”¨
                if ".svg" in ai_image:
                    display_image = DEFAULT_IMAGE
                else:
                    display_image = ai_image
            else:
                # 3. å¦‚æœæ²’æŠ“åˆ°ï¼Œç”¨é è¨­åœ–
                display_image = DEFAULT_IMAGE

            # è™•ç†æ–‡å­—
            if len(raw_address) > 2:
                display_text = f"ğŸ“ {raw_address} | ğŸ“ {summary}"
            else:
                display_text = f"ğŸ“ {summary}"

            if len(display_text) > 60:
                display_text = display_text[:57] + "..."

            results.append({
                "name": shop['name'],
                "address": display_text,     
                "score": "ç²¾é¸",             
                "image": display_image,      # ä½¿ç”¨å‰›å‰›åˆ¤æ–·å®Œçš„åœ–ç‰‡
                "link": get_gmap_link(shop['name'])
            })
        return results
    else:
        return []
